### Reducción de la pérdida: Descenso de gradientes 

Tiempo estimado: 10 minutos

El diagrama de enfoque iterativo (Figura 1) contenía un cuadro verde con afirmaciones sin fundamento llamado "Actualizar parámetros". Ahora reemplazaremos esa solución algorítmica mágica por algo más sustancial.

Supongamos que tuvimos el tiempo y los recursos de procesamiento a fin de calcular la pérdida de todos los valores posibles de w1
. Para el tipo de problemas de regresión que hemos examinado, la representación resultante de pérdida frente a w1
 siempre será convexa. En otras palabras, la trama siempre será en forma de tazón, como la siguiente:
 ![image](https://github.com/jwattspajaro/Machine_Learning/assets/18930760/36bf9c8d-fec8-44d6-a934-f10cc99fb658)
### Figura 2: Los problemas de regresión producen trazados de pérdida convexa en comparación con los de peso.

 

Los problemas convexos tienen solo un mínimo; es decir, solo un lugar en el que la pendiente es exactamente 0. Ese mínimo es donde converge la función de pérdida.

Calcular la función de pérdida para cada valor concebible de w1
en todo el conjunto de datos sería una forma ineficiente de encontrar el punto de convergencia. Examinemos un mejor mecanismo, muy popular en el aprendizaje automático, denominado descenso de gradientes.

La primera etapa en el descenso de gradientes es elegir un valor de inicio (un punto de partida) para w1
. El punto de partida no es muy importante; por lo tanto, muchos algoritmos simplemente se establecen en w1
 o eligen un valor aleatorio. En la siguiente figura, se muestra que elegimos un punto de partida ligeramente mayor que 0:
 ![image](https://github.com/jwattspajaro/Machine_Learning/assets/18930760/93909598-38df-4c1e-9748-a2aaf570868c)

Figura 3: Un punto de partida para el descenso de gradientes.

      Luego, el algoritmo de descenso de gradientes calcula el gradiente de la curva de pérdida en el punto de partida. En la Figura 3, el gradiente de la pérdida es igual a la derivada (pendiente) de la curva y te indica en qué dirección es “más cálido” o “más frío”. Cuando hay varios pesos, el gradiente es un vector de derivadas parciales con respecto a los pesos.

      La matemática relacionada con el aprendizaje automático es fascinante y nos complace que hayas hecho clic en el vínculo para obtener más información. Sin embargo, ten en cuenta que TensorFlow se ocupa de todos los cálculos de gradientes, por lo que no es necesario que comprendas el cálculo que se proporciona aquí.

      ## Derivadas parciales

      Una función multivariable es una función con más de un argumento, como la siguiente:

      f(x,y) = e^(2y)sin(x)

      La derivada parcial ∂f/∂x es la derivada de f considerada como una función de x sola. Para encontrar esta derivada parcial, debes mantener y constante (por lo que f ahora es una función de una variable x) y tomar la derivada regular de f con respecto a x. Por ejemplo, cuando y se fija en 1, la función anterior se vuelve:

      f(x) = e^2sin(x)

      Esto es solo una función de una variable x, cuya derivada es:

      e^2cos(x)

      En general, al pensar en y como fija, la derivada parcial de f con respecto a x se calcula de la siguiente manera:

      ∂f/∂x(x,y) = e^(2y)cos(x)

      Del mismo modo, si x se mantiene fijo, la derivada parcial de f con respecto a y es:

      ∂f/∂y(x,y) = 2e^(2y)sin(x)

      De manera intuitiva, una derivada parcial te indica cuánto cambia la función cuando perturbas un poco una variable. 
      
     En el ejemplo anterior:

      ∂f/∂x(0,1) = e^2 ≈ 7.4

      Por lo tanto, cuando comienzas en (0,1), mantienes y constante y mueves x un poco, 
      f cambia alrededor de 7.4 veces el importe que cambiaste x.

      En el aprendizaje automático, las derivadas parciales se usan mayormente en conjunto con la gradiente de una función.

      ## Gradientes

      El gradiente de una función, denotado de la siguiente manera, 
      es el vector de las derivadas parciales con respecto a todas las variables independientes:

      ∇f

      Por ejemplo, si:

      f(x,y) = e^(2y)sin(x)

      luego:

El gradiente ∇f(x,y) indica la dirección y la magnitud del mayor cambio de la función en el punto (x,y).
En otras palabras, nos proporciona información sobre cómo varía la función en diferentes direcciones 
alrededor de dicho punto.


∇f(x,y) = (∂f/∂x)(x,y), (∂f/∂y)(x,y) = (e^(2y)cos(x), 2e^(2y)sin(x))




      Ten en cuenta lo siguiente:

      - ∇f: Puntos en la dirección del mayor aumento de la función.
      - −∇f: Puntos en la dirección de la mayor disminución de la función.

      La cantidad de dimensiones del vector es igual a la cantidad de variables en la fórmula para f; en otras palabras, 
      el vector se encuentra dentro del espacio del dominio de la función. Por ejemplo, 
      el grafo de la siguiente función f(x,y):

      f(x,y) = 4 + (x−2)^2 + 2y^2

      Cuando se ve en tres dimensiones con z = f(x,y), parece un valle con un mínimo de (2,0,4).
      
 ![image](https://github.com/jwattspajaro/Machine_Learning/assets/18930760/aa580fca-3884-4bef-a670-e00e164ce76c)
 
 El gradiente de f(x,y)
 es un vector bidimensional que te indica en qué dirección(x,y)
 moverte para obtener el aumento máximo de altura. Por lo tanto, el negativo del gradiente te mueve en la dirección de la disminución máxima de la altura. 
 En otras palabras, el negativo del vector de gradiente apunta al valle.
En el aprendizaje automático, los gradientes se usan en el descenso de gradientes. A menudo, 
tenemos una función de pérdida de muchas variables que intentamos minimizar, y tratamos de hacer esto siguiendo el negativo del gradiente de la función.

Ten en cuenta que un gradiente es un vector, por lo que tiene las dos características siguientes:

- una dirección
- una magnitud
El gradiente siempre apunta en la dirección del aumento más empinado de la función de pérdida. 
El algoritmo de descenso de gradientes toma un paso en dirección del gradiente negativo para reducir la pérdida lo más rápido posible

![image](https://github.com/jwattspajaro/Machine_Learning/assets/18930760/64e0f3b8-5039-448b-b3bd-59b0915e6583)

Figura 4: El descenso de gradientes se basa en gradientes negativos.

Para determinar el siguiente punto a lo largo de la curva de la función de pérdida, 
el algoritmo de descenso de gradientes agrega una fracción de la magnitud de la gradiente al punto de partida, 
como se muestra en la siguiente figura:
![image](https://github.com/jwattspajaro/Machine_Learning/assets/18930760/66a712d7-73a3-47cc-a646-50dacad237c5)

Figura 5: Un paso de gradiente nos mueve al siguiente punto en la curva de pérdida.

Luego, el descenso de gradientes repite este proceso y se acerca cada vez más al mínimo.

Nota: Cuando se realiza un descenso de gradientes, generalizamos el proceso anterior para ajustar todos los parámetros del modelo simultáneamente. Por ejemplo, para encontrar los valores óptimos de w1
 y del sesgo b
, calculamos los gradientes con respecto a w1  y b. A continuación, modificamos los valores de w1 y b
 según sus respectivos gradientes. Luego, repetimos estos pasos hasta llegar a la pérdida mínima.
 
**Términos clave**
-  ### [descenso de gradientes](https://developers.google.com/machine-learning/glossary?hl=es-419#gradient_descent)
-  ### [paso](https://developers.google.com/machine-learning/glossary?hl=es-419#step)


      

